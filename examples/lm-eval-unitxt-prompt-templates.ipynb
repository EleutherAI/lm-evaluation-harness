{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Demo] Apply different prompt templates in a dataset\n",
    "\n",
    "TLDR: This demo, create a list of prompts templates using unitxt. Following we create new dataset tasks formated with new prompts to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from lm_eval.prompts.prompt_template_utils import build_prompts_variations_str_template\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Navigate to the root path\n",
    "LM_EVAL_ROOT_PATH = os.path.abspath(os.path.join(current_dir, \"../\"))\n",
    "sys.path.insert(0, LM_EVAL_ROOT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install unitxt package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unitxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the templates variation for multi QA template that can be applied to a given dataset. Initial template is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default template from MMLU\n",
    "multi_qa_template = \"The following are multiple choice questions (with answers) about {topic}.\\n{question}.\\nAnswers: \\n{choices}.\\nAnswer:\"\n",
    "\n",
    "templates = build_prompts_variations_str_template(\n",
    "    template_str=multi_qa_template, dataset_name=\"mmlu\", templates_folder=\"assets\", num_variations=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save raw templates variations\n",
    "hf_dataset_name = \"cais/mmlu\"\n",
    "templates_folder = f\"{LM_EVAL_ROOT_PATH}/examples/assets\"\n",
    "\n",
    "os.makedirs(templates_folder, exist_ok=True)\n",
    "dataset_name = hf_dataset_name.split(\"/\", 1)[1] if \"/\" in hf_dataset_name else \"\"\n",
    "with open(f\"{templates_folder}/{dataset_name}_templates.json\", \"r\") as file:\n",
    "    raw_templates = json.load(file)\n",
    "\n",
    "raw_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unitxt utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitxt import add_to_catalog, get_from_catalog\n",
    "from unitxt.blocks import LoadHF, Set, TaskCard\n",
    "from unitxt.splitters import RenameSplits\n",
    "from unitxt.templates import MultipleChoiceTemplate, TemplatesList\n",
    "\n",
    "\n",
    "def write_card_yaml(filename, data):\n",
    "    import yaml\n",
    "\n",
    "    with open(filename, \"w\") as stream:\n",
    "        yaml.dump(data, stream, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the create using unitxt Template Object and add into the unitxt local catalog\n",
    "\n",
    "prompt_templates = {\"mmlu_with_topic\": raw_templates}\n",
    "template_handles = []\n",
    "\n",
    "for template_type, template_group in prompt_templates.items():\n",
    "    for index, input_format in enumerate(template_group):\n",
    "        template = MultipleChoiceTemplate(\n",
    "            input_format=input_format,\n",
    "            target_field=\"answer\",\n",
    "            choices_separator=\"\\n\",\n",
    "            postprocessors=[\"processors.first_character\"],\n",
    "        )\n",
    "        template_handle = f\"templates.qa.multiple_choice.{template_type}.pt_variation_{index}\"\n",
    "        template_handles.append(template_handle)\n",
    "        add_to_catalog(template, template_handle, overwrite=True)\n",
    "\n",
    "# save the template handles as a list in the catalog\n",
    "templates_list_catalog_name = \"templates.qa.multiple_choice.mmlu_with_topic.all_pt_variations\"\n",
    "add_to_catalog(\n",
    "    artifact=TemplatesList(template_handles),\n",
    "    name=templates_list_catalog_name,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the templates list from the catalog\n",
    "_templates_list: TemplatesList = get_from_catalog(templates_list_catalog_name)  # catalog_path=my_unitxt_catalog\n",
    "\n",
    "template_list = []\n",
    "for template in _templates_list.items:\n",
    "    template_list.append(template.get_pretty_print_name())\n",
    "\n",
    "template_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of mmlu tasks:\n",
    "dataset_name = \"mmlu\"\n",
    "\n",
    "# fmt: off\n",
    "subtasks = [\n",
    "    \"abstract_algebra\",\"anatomy\",\"astronomy\",\"business_ethics\",\"clinical_knowledge\",\n",
    "    \"college_biology\",\"college_chemistry\",\"college_computer_science\",\"college_mathematics\",\n",
    "    \"college_medicine\",\"college_physics\",\"computer_security\",\"conceptual_physics\",\"econometrics\",\n",
    "    \"electrical_engineering\",\"elementary_mathematics\",\"formal_logic\",\"global_facts\",\n",
    "    \"high_school_biology\",\"high_school_chemistry\",\"high_school_computer_science\",\n",
    "    \"high_school_european_history\",\"high_school_geography\",\"high_school_government_and_politics\",\n",
    "    \"high_school_macroeconomics\",\"high_school_mathematics\",\"high_school_microeconomics\",\n",
    "    \"high_school_physics\",\"high_school_psychology\",\"high_school_statistics\",\"high_school_us_history\",\n",
    "    \"high_school_world_history\",\"human_aging\",\"human_sexuality\",\"international_law\",\"jurisprudence\",\n",
    "    \"logical_fallacies\",\"machine_learning\",\"management\",\"marketing\",\"medical_genetics\",\"miscellaneous\",\n",
    "    \"moral_disputes\",\"moral_scenarios\",\"nutrition\",\"philosophy\",\"prehistory\",\"professional_accounting\",\n",
    "    \"professional_law\",\"professional_medicine\",\"professional_psychology\",\"public_relations\",\"security_studies\",\n",
    "    \"sociology\",\"us_foreign_policy\",\"virology\",\"world_religions\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Up Your Custom LM-Eval Unitxt Tasks Directory**\n",
    "\n",
    "reference: https://www.unitxt.ai/en/latest/docs/lm_eval.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder to save the yaml data cards to be used with lm eval\n",
    "os.makedirs(f\"{LM_EVAL_ROOT_PATH}/examples/pt_variations_tasks\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After create the tasks directory, run the following code to save the Unitxt configuration file in your tasks directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c 'from lm_eval.tasks.unitxt import task; import os.path; \\\n",
    "    print(\"class: !function \" + task.__file__.replace(\"task.py\", \"task.Unitxt\"))' > ./pt_variations_tasks/unitxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now have a unitxt file in your `./pt_variations_tasks` directory that defines the integration with your local virtual environment. This step should be performed once. Note that when changing virtual environments, you will need to update it using the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create my recipe, with the template list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card = f\"cards.{dataset_name}.{subtasks[0]} \"\n",
    "template = template_list\n",
    "\n",
    "data = {\n",
    "    \"task\": \"mmlu_variation_example\",\n",
    "    \"include\": \"unitxt\",\n",
    "    \"recipe\": f\"card={card},template={template[2]}\",\n",
    "}\n",
    "\n",
    "write_card_yaml(f\"{LM_EVAL_ROOT_PATH}/examples/pt_variations_tasks/{dataset_name}_variation_example.yaml\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute your newly constructed task, with your selected model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/flan-t5-base --limit 10\\\n",
    "    --device cpu --tasks mmlu_variation_example --include_path pt_variations_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
