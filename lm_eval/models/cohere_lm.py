"""Module with Cohere API-based language model."""

import os
import transformers
from lm_eval.base import BaseLM
from lm_eval import utils
from tqdm import tqdm
import cohere


class CohereLM(BaseLM):
    """Language model accessed via Cohere API."""

    REQ_CHUNK_SIZE = 20

    def __init__(self, model="medium", truncate=False, max_retries=100, timeout=30):
        """Language model accessed via Cohere API.

        The API is documented here: https://docs.cohere.ai/reference/generate.
        This class is based on the gpt3.py model with the OpenAI API.

        Imortant: in order to use this LM you need to set the environment variable
        COHERE_API_SECRET_KEY to your Cohere API key.

        :param model: str
            The type of Cohere model to be used, can be either `medium` or `xlarge`.
            Deaults to `medium`.
        :param truncate: bool
            Truncate input if too long (if False and input is too long, throw error)
        :param max_retries: int
            Maximum number of retries for each API call.
        :param timeout: int
            Timeout for each API call in seconds.
        """
        super().__init__()

        self.model = model

        # Set up tokenizer
        self.tokenizer = transformers.GPT2TokenizerFast.from_pretrained("gpt2")
        self.vocab_size = self.tokenizer.vocab_size
        # to make the annoying "Using pad_token, but it is not set yet." error go away
        self.tokenizer.pad_token = "<|endoftext|>"
        assert self.tokenizer.encode("hello\n\nhello") == [31373, 198, 198, 31373]
        self.truncate = truncate
        self.end_of_text_token_id = self.tokenizer.convert_tokens_to_ids(
            ["<|endoftext|>"]
        )[0]

        # Set up Cohere API client
        api_key = os.environ["COHERE_API_SECRET_KEY"]
        self.cohere_client = cohere.Client(
            api_key, max_retries=max_retries, timeout=timeout
        )

    @property
    def eot_token_id(self):
        return self.tokenizer.eos_token_id

    @property
    def max_length(self):
        # Note: the OpenAI API supports up to 2049 tokens, with the first token being the first input token
        return 2048

    @property
    def max_gen_toks(self):
        return 256

    @property
    def batch_size(self):
        # Isn't used because we override _loglikelihood_tokens
        raise NotImplementedError()

    @property
    def device(self):
        # Isn't used because we override _loglikelihood_tokens
        raise NotImplementedError()

    def tok_encode(self, string: str):
        return self.tokenizer.encode(string, add_special_tokens=False)

    def tok_decode(self, tokens):
        return self.tokenizer.decode(tokens)

    def _loglikelihood_tokens(self, requests, disable_tqdm=False):
        """Compute log-likelihood of generating a continuation from a context.

        The log likelihood of tokens can be obtained directly from the API.

        :param requests: list
            A list with elements ((context, continuation), context_enc, continuation_enc)
            Where
            context: str
                Context string.
            continuation: str
                The continuation as tokens over which log likelihood
                will be calculated.
            *_enc: the encoded (tokenised) version of context und continuations.
        :return: list
            A list of pairs (logprob, isgreedy)
            logprob: float
                The log probability of `continuation`
            isgreedy:
                Whether `continuation` would be generated by greedy sampling from `context`
        """

        res = []

        # We first create datastructure for the requests
        # that allows us to:
        # 1) avoid duplicate requests to minimise API calls
        # 2) reorder requests to start with the longest requests to
        #   have out-of-memory errors at the beginning of loop.
        #
        # We use the following two methods:
        #
        # re_ord.get_reordered(): returns
        #   a list of unique context, continuation pairs
        #   (where any split between context and contuation is considered
        #   identical). Ordered by descending length of context+continuation
        # re_ord.get_original(res): given an array res of the same
        #   len as get_reordered(), returns the original array with
        #   res array elements switched in for index matching
        #   original values.

        def _collate(val):
            # makes the reorderer sort by descending
            # length of context+continuation
            # note that tokens are not used here,
            # thus using str length
            contin_str = val[0][1]
            context_str = val[0][0]
            combined_str = context_str + contin_str
            return -len(combined_str), tuple(combined_str)

        re_ord = utils.Reorderer(requests, _collate)

        # iterate over chunks (i.e. subsets) of reordered requests
        for chunk in tqdm(
            list(utils.chunks(re_ord.get_reordered(), self.REQ_CHUNK_SIZE)),
            disable=disable_tqdm,
        ):
            for (context, continuation), _, _ in chunk:

                # get response from cohere API and retry later if error is thrown
                response = self.cohere_client.generate(
                    model=self.model,  # "medium" or "xlarge"
                    prompt=context + continuation,
                    max_tokens=0,
                    temperature=0.0,
                    return_likelihoods="ALL",
                    # truncate any tokens from beginning
                    # over the limit of 2048 of API
                    truncate="START",
                )

                # Check if greedy

                # Cohere's API does not provide a logprobs argument
                # (like OpenAI's), thus we need a second API call to
                # check if the greedy continuation is the same as the
                # evaluated continuation.
                context_tokens = self.cohere_client.tokenize(text=context)
                context_token_len = len(context_tokens.tokens)
                overall_token_len = len(response.generations[0].token_likelihoods)
                continuation_token_len = overall_token_len - context_token_len

                greedy_response = self.cohere_client.generate(
                    model=self.model,  # "medium" or "xlarge"
                    prompt=context,
                    max_tokens=continuation_token_len,
                    temperature=0.0,
                    # truncate any tokens from beginning
                    # over the limit of 2048 of API
                    truncate="START",
                )
                is_greedy = (
                    response.generations[0].text == greedy_response.generations[0].text
                )

                # compute logprob of continuation
                regular_likelihoods = response.generations[0].token_likelihoods
                continuation_logprob = sum(
                    [
                        token.likelihood
                        for token in regular_likelihoods[context_token_len:]
                    ]
                )

                answer = continuation_logprob, is_greedy
                res.append(answer)

                # TODO: does this cache key logic make any sense?
                # The logic is copied from gpt3.py LM class.
                cache_key = (context, continuation)
                if cache_key is not None:
                    self.cache_hook.add_partial("loglikelihood", cache_key, answer)

        return re_ord.get_original(res)

    def greedy_until(self, requests):
        """Generate greedily until a stopping sequence

        :param requests: list
            A list of pairs (context, until)
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list
            A list of strings continuation
            continuation: str
                The generated continuation.
        """

        if not requests:
            return []
        res = []

        def _collate(x):
            toks = self.tok_encode(x[0])
            return len(toks), x[0]

        re_ord = utils.Reorderer(requests, _collate)

        def sameuntil_chunks(xs, size):
            """Iterable that returns sublists of xs of max len `size`
            and with identical until values.
            """
            ret = []
            lastuntil = xs[0][1]
            for x in xs:
                if len(ret) >= size or x[1] != lastuntil:
                    yield ret, lastuntil
                    ret = []
                    lastuntil = x[1]
                ret.append(x)

            if ret:
                yield ret, lastuntil

        for chunk, until in tqdm(
            list(sameuntil_chunks(re_ord.get_reordered(), self.REQ_CHUNK_SIZE))
        ):
            for context, _ in chunk:
                response = cohere_api_call(
                    self.cohere_client,
                    kwargs=dict(
                        model=self.model,  # "medium" or "xlarge"
                        prompt=context,
                        max_tokens=self.max_gen_toks,
                        temperature=0.0,
                        return_likelihoods="ALL",
                        # truncate any tokens from beginning
                        # over the limit of 2048 of API
                        truncate="START",
                        # end sequences are NOT included in returned text
                        end_sequences=until,
                    ),
                )
                gen_text = response.generations[0].text

                # partial caching
                self.cache_hook.add_partial("greedy_until", (context, until), gen_text)

                res.append(gen_text)

        return re_ord.get_original(res)

    def _model_call(self, inps):
        # Isn't used because we override _loglikelihood_tokens
        raise NotImplementedError()

    def _model_generate(self, context, max_length, eos_token_id):
        # Isn't used because we override greedy_until
        raise NotImplementedError()
