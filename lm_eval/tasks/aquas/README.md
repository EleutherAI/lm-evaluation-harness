# AQuAS

### Paper

Title: `Abstractive Question Answering in Spanish (AQuAS)`

`EN:` AQuAS is a high-quality dataset manually created by two computational linguists with the objective of evaluating language models on the Abstractive Question-Answering task in Spanish.

`ES:` AQuAS es un dataset de alta calidad realizado de forma manual por dos lingüistas computacionales con el fin de evaluar modelos de lenguaje en la tarea de Question-Answering Abstractivo en español.

Homepage: https://huggingface.co/datasets/IIC/AQuAS


### Citation

```
@misc {Instituto de Ingeniería del Conocimiento (IIC),
    author       = { {Instituto de Ingeniería del Conocimiento} },
    title        = { Abstractive Question-Answering in Spanish (AQuAS) Dataset },
    year         = 2024,
    url          = { https://huggingface.co/datasets/IIC/AQuAS },
    doi          = { 10.57967/hf/2043 },
    publisher    = { Hugging Face }
}
```

### Groups and Tasks

#### Groups

* Open Spanish LLM Leaderboard

#### Tasks

* `aquas`

### Checklist

For adding novel benchmarks/datasets to the library:
* [x] Is the task an existing benchmark in the literature?
  * [ ] Have you referenced the original paper that introduced the task?
  * [ ] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?


If other tasks on this dataset are already supported:
* [ ] Is the "Main" variant of this task clearly denoted?
* [ ] Have you provided a short sentence in a README on what each new variant adds / evaluates?
* [ ] Have you noted which, if any, published evaluation setups are matched by this variant?
