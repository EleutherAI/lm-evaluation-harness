# Aya Redteaming

## Paper

Title: `The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm`

Abstract: https://arxiv.org/abs/2406.18682

Homepage: https://huggingface.co/datasets/CohereLabs/aya_redteaming

Human-annotated multilingual red-teaming dataset consisting of harmful prompts in 8 languages across 9 different categories of harm with explicit labels for "global" and "local" harm.

### Citation

```text
@misc{aakanksha2024multilingualalignmentprismaligning,
      title={The Multilingual Alignment Prism: Aligning Global and Local Preferences to Reduce Harm}, 
      author={Aakanksha and Arash Ahmadian and Beyza Ermis and Seraphina Goldfarb-Tarrant and Julia Kreutzer and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2406.18682},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18682}, 
}
```

### Implementation

In the paper, the authors evaluate the toxicity of models by asking GPT-4 a binary question about the completion’s harmfulness. In this implementation, we pose the same type of question to open-source models loaded with `transformers`.

> Appendix F: LLMs as evaluators
> To measure the performance of our models in the open-ended generation tasks, we use GPT4 as the evaluator. Modern evaluation methods are increasingly using powerful LLMs to assess the quality of responses generated by other (or sometimes even the same) such language models. For the measurement of safety performance, we use a “standalone” grading scheme, where we present a prompt and completion to GPT-4 at once and pose a binary question about the completion’s harmfulness. For the measurement of general performance, we follow a “pairwise” preference scheme, where we present a prompt and two completions at once —- one from the base model and another from the fine-tuned model— and ask about the model’s general preference between the two responses with the option to also tie the two.

### Groups, Tags, and Tasks

#### Groups

* `aya_redteaming`: `All languages`

#### Tasks

* `aya_redteaming_arabic`: `Arabic prompts`
* `aya_redteaming_english`: `English prompts`
* `aya_redteaming_filipino`: `Filipino prompts`
* `aya_redteaming_french`: `French prompts`
* `aya_redteaming_hindi`: `Hindi prompts`
* `aya_redteaming_russian`: `Russian prompts`
* `aya_redteaming_serbian`: `Serbian prompts`
* `aya_redteaming_spanish`: `Spanish prompts`

### Checklist

For adding novel benchmarks/datasets to the library:

* [x] Is the task an existing benchmark in the literature?
  * [x] Have you referenced the original paper that introduced the task?
  * [x] If yes, does the original paper provide a reference implementation? If so, have you checked against the reference implementation and documented how to run such a test?

If other tasks on this dataset are already supported:

* [ ] Is the "Main" variant of this task clearly denoted?
* [ ] Have you provided a short sentence in a README on what each new variant adds / evaluates?
* [ ] Have you noted which, if any, published evaluation setups are matched by this variant?
