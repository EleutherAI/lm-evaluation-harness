"""
MLSum:

We present MLSUM, the first large-scale MultiLingual SUMmarization dataset.
Obtained from online newspapers, it contains 1.5M+ article/summary pairs in five
different languages -- namely, French, German, Spanish, Russian, Turkish.
Together with English newspapers from the popular CNN/Daily mail dataset,
the collected data form a large scale multilingual dataset which can enable
new research directions for the text summarization community.
We report cross-lingual comparative analyses based on state-of-the-art systems.
These highlight existing biases which motivate the use of a multi-lingual dataset.

Paper: https://www.aclweb.org/anthology/2020.emnlp-main.647/

Dataset: https://huggingface.co/datasets/mlsum

Prompts are translated with Google Translate.

"""
import datasets
from lm_eval.base import rf, Task
from functools import partial


def _mlsum_metric(predictions, references):
    summarization_metric = datasets.load_metric("rouge")
    return summarization_metric.compute(predictions=predictions, references=references)


def _mlsum_agg(key, items):
    predictions, references = zip(*items)
    return _mlsum_metric(predictions=predictions, references=references)[
        key
    ].mid.fmeasure


class MLSumBase(Task):
    VERSION = None
    DATASET_PATH = "mlsum"
    DATASET_NAME = None
    PROMPT_INSTRUCTION = "Article: "
    PROMPT_END = "TL;DR:"

    def has_training_docs(self):
        return True

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def training_docs(self):
        return self.dataset["train"]

    def validation_docs(self):
        return self.dataset["validation"]

    def test_docs(self):
        return self.dataset["test"]

    def doc_to_text(self, doc):
        return self.PROMPT_INSTRUCTION + doc["text"] + "\n\n" + self.PROMPT_END

    def doc_to_target(self, doc):
        summary = doc["summary"]
        return " " + summary

    def construct_requests(self, doc, ctx):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        continuation = rf.greedy_until(ctx, ["\n"])
        return continuation

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        continuation = results

        predictions = {
            "id": doc["topic"],
            "prediction_text": continuation,
            "no_answer_probability": 0,
        }

        references = {
            "id": doc["topic"],
            "answers": [doc["summary"]],
        }

        return {
            "rouge1": (predictions, references),
            "rouge2": (predictions, references),
            "rougeL": (predictions, references),
        }

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        return {
            "rouge1": partial(_mlsum_agg, "rouge1"),
            "rouge2": partial(_mlsum_agg, "rouge2"),
            "rougeL": partial(_mlsum_agg, "rougeL"),
        }

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {"rouge1": True, "rouge2": True, "rougeL": True}


class MLSumDE(MLSumBase):
    DATASET_NAME = "de"
    PROMPT_INSTRUCTION = "Artikel: "
    PROMPT_END = "Zusammenfassung: "


class MLSumES(MLSumBase):
    DATASET_NAME = "es"
    PROMPT_INSTRUCTION = "Artículo: "
    PROMPT_END = "Resumen: "


class MLSumFR(MLSumBase):
    DATASET_NAME = "fr"
    PROMPT_INSTRUCTION = "Article: "
    PROMPT_END = "Sommaire: "


class MLSumRU(MLSumBase):
    DATASET_NAME = "ru"
    PROMPT_INSTRUCTION = "Статья: "
    PROMPT_END = "Резюме: "


class MLSumTU(MLSumBase):
    DATASET_NAME = "tu"
    PROMPT_INSTRUCTION = "Madde: "
    PROMPT_END = "Özet: "


LANGS = ["de", "es", "fr", "ru", "tu"]

LANG_CLASSES = [MLSumDE, MLSumES, MLSumFR, MLSumRU, MLSumTU]


def construct_tasks():
    tasks = {}
    for lang, lang_class in zip(LANGS, LANG_CLASSES):
        tasks[f"mlsum_{lang}"] = lang_class
    return tasks
