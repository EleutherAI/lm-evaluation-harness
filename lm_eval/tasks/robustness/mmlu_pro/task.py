import copy
import re
import string
from typing import List
import sys
import os

import json
import numpy as np


from lm_eval.api.instance import Instance
from lm_eval.api.task import ConfigurableTask
from lm_eval.utils import eval_logger



class MmluPro(ConfigurableTask):
    VERSION = 0
    DATASET_PATH = "TIGER-Lab/MMLU-Pro"
    DATASET_NAME = "default"
    validation_split = "validation"

    def __init__(self, **kwargs):
        try:
            template_file = os.path.join(os.path.dirname(__file__), "prompt_templates.json")
            with open(template_file) as f:
                self.prompt_templates = json.load(f)["v0.2_templates"] #todo
        except FileNotFoundError:
            eval_logger.error("Prompt templates not found")
            sys.exit()

        self.labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
        self.separators = ["<extra_id_1>", "<|eot_id|>", "\n", " ", "\\n"]

        super().__init__(config={"metadata": {"version": self.VERSION}})
        
    def __repeat_doc_add_prompt_id(self, batched_docs):
        n = len(self.prompt_templates)
        initial_len = len(next(iter(batched_docs.values())))

        def __repeat_elements(lst, n): #todo move to utils
            result = []
            for element in lst:
                result.extend([element] * n)
            return result
        result = {key: __repeat_elements(values, n) for key, values in batched_docs.items()}
        result["prompt_id"] = list(range(n)) * initial_len
        return result

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def validation_docs(self):
        new_val_set = self.dataset["validation"].map(self.__repeat_doc_add_prompt_id, batched=True)
        return new_val_set
    
    def test_docs(self):
        new_val_set = self.dataset["test"].map(self.__repeat_doc_add_prompt_id, batched=True)
        return new_val_set
    
    def doc_to_text(self, doc):
        # uppercase letters
        _l = string.ascii_uppercase
        options = "\n".join([f"{_l[i]}) {doc['options'][i]}" for i in range(len(doc["options"]))])
        return self.prompt_templates[doc["prompt_id"]].format(question=doc["question"], options=options)
    
    def doc_to_target(self, doc):
        return doc["answer"]

    def construct_requests(self, doc, ctx, **kwargs):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        
        return [
            Instance(
                request_type="generate_until",
                doc=doc,
                arguments=(ctx, {"until": ["\n"], "max_gen_toks": 512}),
                idx=0,
                **kwargs,
            )
        ]

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        final_answer = [self.postprocess_pred(result) for result in results]
        category = doc["category"]
        prompt_id = str(doc["prompt_id"])

        return {f"accuracy_{prompt_id}": int(final_answer[0].lower() == doc["answer"].lower())}


    def postprocess_pred(self, predict_str):
        """
        Todo: make code simpler
        """
        for l in self.labels:
            if predict_str.startswith(f"{l}:"):
                return l
            if predict_str.startswith(f"{l}\n"):
                return l
            if f" {l}: " in predict_str:
                return l

        if 'which corresponds to' in predict_str:
            predict_str = predict_str.split('which corresponds to')[-1].strip()
            for word in predict_str.split():
                only_letters = re.sub(r"[^a-zA-Z]", "", word).strip()
                for choice in ["A", "B", "C", "D", "E"]:
                    if only_letters == choice:
                        return only_letters

        predict_str = predict_str.strip()
        to_split = ['the correct answer is', 'the correct response is', 'the correct option is', 'the most accurate answer is',
                    'the best answer here is', 'the best answer is', 'the answer must be', 'the most accurate response is',
                    'the most fitting response is', 'the most fitting answer is', 'the answer is', 'answer: ' ]
        for answer_str in to_split:
            if answer_str in predict_str.lower():
                predict_str = predict_str.lower().split(answer_str)
                predict_str = [i for i in predict_str if len(i) > 0]
                if len(predict_str) > 0:
                    predict_str = predict_str[-1].upper()
                else:
                    return "EMPTY"
                predict_str = predict_str.strip(string.punctuation).strip()

        for separator in self.separators:
            if separator in predict_str:
                predict_str = predict_str.split(separator)[0].strip()

        delimiters = [" ", ",", "."]
        quotes = ["'", '"', "'", "`", "`", "."]
        # if LABEL_TO_ID[task_name] doesn't contain any quotes, remove them from predict_str
        if not any([quote in "".join(self.labels) for quote in quotes]):
            for quote in quotes:
                predict_str = predict_str.replace(quote, "")

        # remove repeated labels while making sure only the label is repeated
        for label in self.labels:
            label_count = predict_str.count(label)
            if label_count > 1:
                for delimiter in delimiters:
                    if delimiter in predict_str:
                        repeated_label = delimiter.join([label] * label_count)
                        if repeated_label == predict_str:
                            predict_str = predict_str.split(delimiter)[0]
                            break
        replace_dict = {',':'', '\\_': '_', '.':'', ':':''}
        for key, value in replace_dict.items():
            predict_str = predict_str.replace(key, value)

        predict_str = predict_str.strip(string.punctuation).strip()
        return predict_str

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        aggregations = {}
        for i in range(10):
            aggregations[f"accuracy_{i}"] = np.mean
        return aggregations

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "accuracy": True,  # Exact match (the normalized answer exactly match the gold answer
        }


class MmluProSameOption(ConfigurableTask):
    VERSION = 0
    DATASET_PATH = "TIGER-Lab/MMLU-Pro"
    DATASET_NAME = "default"
    validation_split = "validation"

    def __init__(self, **kwargs):
        try:
            template_file = os.path.join(os.path.dirname(__file__), "prompt_templates.json")
            with open(template_file) as f:
                self.prompt_templates = json.load(f)["v0.2_templates"] #todo
        except FileNotFoundError:
            eval_logger.error("Prompt templates not found")
            sys.exit()

        self.labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
        self.separators = ["<extra_id_1>", "<|eot_id|>", "\n", " ", "\\n"]

        super().__init__(config={"metadata": {"version": self.VERSION}})

    # a function that receives a batch of documents with keys "question", "options", "answer", "answer_id" and 
    # returns a new bigger batch of documents with the same keys where every question is repeated and the correct 
    # answer is swapped with all possible options
    def __repeat_doc_swap_correct_answer(self, batched_docs):
        initial_len = len(next(iter(batched_docs.values())))
        keys = list(batched_docs.keys())
        new_batched_docs = {key: [] for key in keys}
        new_batched_docs["always_same_option"] = []
        
        for doc_ind in range(initial_len):
            for label_ind, label in enumerate(self.labels):
                for key in keys:
                    new_batched_docs[key].append(copy.deepcopy(batched_docs[key][doc_ind]))
                    if label_ind < len(batched_docs["options"][doc_ind]):
                        if key == "options":
                            # Swap correct answer with label_ind option
                            new_batched_docs[key][-1][label_ind] = batched_docs["options"][doc_ind][batched_docs["answer_index"][doc_ind]]
                            new_batched_docs[key][-1][batched_docs["answer_index"][doc_ind]] = batched_docs["options"][doc_ind][label_ind]
                        
                        if key == "answer_index":
                            new_batched_docs[key][-1] = label_ind

                        if key == "answer":
                            new_batched_docs[key][-1] = label

                new_batched_docs["always_same_option"].append(label)
        return new_batched_docs

    def has_training_docs(self):
        return False

    def has_validation_docs(self):
        return True

    def has_test_docs(self):
        return True

    def validation_docs(self):
        new_val_set = self.dataset["validation"].map(self.__repeat_doc_swap_correct_answer, batched=True)
        return new_val_set
    
    def test_docs(self):
        new_val_set = self.dataset["test"].map(self.__repeat_doc_swap_correct_answer, batched=True)
        return new_val_set
    
    def doc_to_text(self, doc):
        # uppercase letters
        _l = string.ascii_uppercase
        options = "\n".join([f"{_l[i]}: {doc['options'][i]}" for i in range(len(doc["options"]))])
        prompt = self.prompt_templates[doc["prompt_id"]] if "prompt_id" in doc else self.prompt_templates[0]
        return prompt.format(question=doc["question"], options=options)
    
    def doc_to_target(self, doc):
        return doc["answer"]

    def construct_requests(self, doc, ctx, **kwargs):
        """Uses RequestFactory to construct Requests and returns an iterable of
        Requests which will be sent to the LM.

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param ctx: str
            The context string, generated by fewshot_context. This includes the natural
            language description, as well as the few shot examples, and the question
            part of the document for `doc`.
        """
        
        return [
            Instance(
                request_type="generate_until",
                doc=doc,
                arguments=(ctx, {"until": ["\n"], "max_gen_toks": 512}),
                idx=0,
                **kwargs,
            )
        ]

    def process_results(self, doc, results):
        """Take a single document and the LM results and evaluates, returning a
        dict where keys are the names of submetrics and values are the values of
        the metric for that one document

        :param doc:
            The document as returned from training_docs, validation_docs, or test_docs.
        :param results:
            The results of the requests created in construct_requests.
        """
        final_answer = [self.postprocess_pred(result) for result in results]
        category = doc["category"]
        always_same_opt = doc["always_same_option"]

        return {f"accuracy_{always_same_opt}": int(final_answer[0].lower() == doc["answer"].lower())}


    def postprocess_pred(self, predict_str):
        """
        Todo: make code simpler
        """
        for l in self.labels:
            if predict_str.startswith(f"{l}:"):
                return l
            if predict_str.startswith(f"{l}\n"):
                return l
            if f" {l}: " in predict_str:
                return l

        if 'which corresponds to' in predict_str:
            predict_str = predict_str.split('which corresponds to')[-1].strip()
            for word in predict_str.split():
                only_letters = re.sub(r"[^a-zA-Z]", "", word).strip()
                for choice in ["A", "B", "C", "D", "E"]:
                    if only_letters == choice:
                        return only_letters

        predict_str = predict_str.strip()
        to_split = ['the correct answer is', 'the correct response is', 'the correct option is', 'the most accurate answer is',
                    'the best answer here is', 'the best answer is', 'the answer must be', 'the most accurate response is',
                    'the most fitting response is', 'the most fitting answer is', 'the answer is', 'answer: ' ]
        for answer_str in to_split:
            if answer_str in predict_str.lower():
                predict_str = predict_str.lower().split(answer_str)
                predict_str = [i for i in predict_str if len(i) > 0]
                if len(predict_str) > 0:
                    predict_str = predict_str[-1].upper()
                else:
                    return "EMPTY"
                predict_str = predict_str.strip(string.punctuation).strip()

        for separator in self.separators:
            if separator in predict_str:
                predict_str = predict_str.split(separator)[0].strip()

        delimiters = [" ", ",", "."]
        quotes = ["'", '"', "'", "`", "`", "."]
        # if LABEL_TO_ID[task_name] doesn't contain any quotes, remove them from predict_str
        if not any([quote in "".join(self.labels) for quote in quotes]):
            for quote in quotes:
                predict_str = predict_str.replace(quote, "")

        # remove repeated labels while making sure only the label is repeated
        for label in self.labels:
            label_count = predict_str.count(label)
            if label_count > 1:
                for delimiter in delimiters:
                    if delimiter in predict_str:
                        repeated_label = delimiter.join([label] * label_count)
                        if repeated_label == predict_str:
                            predict_str = predict_str.split(delimiter)[0]
                            break
        replace_dict = {',':'', '\\_': '_', '.':'', ':':''}
        for key, value in replace_dict.items():
            predict_str = predict_str.replace(key, value)

        predict_str = predict_str.strip(string.punctuation).strip()
        return predict_str

    def aggregation(self):
        """
        :returns: {str: [float] -> float}
            A dictionary where keys are the names of submetrics and values are
            functions that aggregate a list of metrics
        """
        aggregations = {}
        for label in self.labels:
            aggregations[f"accuracy_{label}"] = np.mean
        return aggregations

    def higher_is_better(self):
        """
        :returns: {str: bool}
            A dictionary where keys are the names of submetrics and values are
            whether a higher value of the submetric is better
        """
        return {
            "accuracy": True,  # Exact match (the normalized answer exactly match the gold answer
        }

