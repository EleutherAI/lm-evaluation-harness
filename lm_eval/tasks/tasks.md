
# Tasks

 A list of supported tasks can be viewed with `lm-eval --tasks list`

| Task Family        | Description                                                                               | Dataset Link          |
|--------------------|-------------------------------------------------------------------------------------------|-----------------------|
| aclue              | Tasks focusing on ancient Chinese language understanding and cultural aspects.            | [Link](#)             |
| aexams             | Tasks related to various academic exams covering a range of subjects.                     | [Link](#)             |
| agieval            | Tasks involving historical data or questions related to history and historical texts.     | [Link](#)             |
| ammlu              | Tasks involving multiple choice questions across various academic disciplines.            | [Link](#)             |
| anli               | Adversarial natural language inference tasks designed to test model robustness.           | [Link](#)             |
| arc                | Tasks involving complex reasoning over a diverse set of questions.                        | [Link](#)             |
| arithmetic         | Tasks involving numerical computations and arithmetic reasoning.                          | [Link](#)             |
| asdiv              | Tasks involving arithmetic and mathematical reasoning challenges.                         | [Link](#)             |
| babi               | Tasks designed as question and answering challenges based on simulated stories.           | [Link](#)             |
| basqueglue         | Tasks designed to evaluate language understanding in Basque language.                     | [Link](#)             |
| bbh                | Tasks focused on deep semantic understanding through hypothesization and reasoning.       | [Link](#)             |
| benchmarks         | General benchmarking tasks that test a wide range of language understanding capabilities. | [Link](#)             |
| belebele           | Language understanding tasks in a variety of languages and scripts.                       | [Link](#)             |
| bigbench           | Broad tasks from the BIG-bench benchmark designed to push the boundaries of large models. | [Link](#)             |
| blimp              | Tasks testing grammatical phenomena to evaluate language model's linguistic capabilities. | [Link](#)             |
| ceval              | Tasks that evaluate language understanding and reasoning in an educational context.       | [Link](#)             |
| cmmlu              | Multi-subject multiple choice question tasks for comprehensive academic assessment.       | [Link](#)             |
| code_x_glue        | Tasks that involve understanding and generating code across multiple programming languages.| [Link](#)          |
| coqa               | Conversational question answering tasks to test dialog understanding.                    | [Link](#)             |
| crows_pairs        | Tasks designed to test model biases in various sociodemographic groups.                   | [Link](#)             |
| csatqa             | Tasks related to SAT and other standardized testing questions for academic assessment.    | [Link](#)             |
| drop               | Tasks requiring numerical reasoning, reading comprehension, and question answering.       | [Link](#)             |
| eq_bench           | Tasks focused on equality and ethics in question answering and decision-making.          | [Link](#)             |
| eus_exams          | Tasks based on various professional and academic exams in the Basque language.            | [Link](#)             |
| eus_proficiency    | Tasks designed to test proficiency in the Basque language across various topics.          | [Link](#)             |
| eus_reading        | Reading comprehension tasks specifically designed for the Basque language.                | [Link](#)             |
| eus_trivia         | Trivia and knowledge testing tasks in the Basque language.                                | [Link](#)             |
| fld                | Tasks involving free-form and directed dialogue understanding.                            | [Link](#)             |
| french_bench       | Set of tasks designed to assess language model performance in French.                     | [Link](#)             |
| glue               | General Language Understanding Evaluation benchmark to test broad language abilities.    | [Link](#)             |
| gpqa               | Tasks designed for general public question answering and knowledge verification.         | [Link](#)             |
| gsm8k              | A benchmark of grade school math problems aimed at evaluating reasoning capabilities.     | [Link](#)             |
| haerae             | Tasks focused on assessing detailed factual and historical knowledge.                     | [Link](#)             |
| headqa             | A high-level education-based question answering dataset to test specialized knowledge.    | [Link](#)             |
| hellaswag          | Tasks to predict the ending of stories or scenarios, testing comprehension and creativity.| [Link](#)             |
| hendrycks_ethics   | Tasks designed to evaluate the ethical reasoning capabilities of models.                  | [Link](#)             |
| ifeval             | Interactive fiction evaluation tasks for narrative understanding and reasoning.           | [Link](#)             |
| kmmlu              | Knowledge-based multi-subject multiple choice questions for academic evaluation.          | [Link](#)             |
| kobest             | A collection of tasks designed to evaluate understanding in Korean language.              | [Link](#)             |
| kormedmcqa         | Medical question answering tasks in Korean to test specialized domain knowledge.          | [Link](#)             |
| lambada            | Tasks designed to predict the endings of text passages, testing language prediction skills.| [Link](#)           |
| logiqa             | Logical reasoning tasks requiring advanced inference and deduction.                       | [Link](#)             |
| mathqa             | Question answering tasks involving mathematical reasoning and problem-solving.            | [Link](#)             |
| medmcqa            | Medical multiple choice questions assessing detailed medical knowledge.                   | [Link](#)             |
| minerva_math       | Mathematics-focused tasks requiring numerical reasoning and problem-solving skills.       | [Link](#)             |
| mmlu               | Massive Multitask Language Understanding benchmark for broad domain language evaluation.  | [Link](#)             |
| model_written_evals| Evaluation tasks based on outputs written by models, testing generation quality.          | [Link](#)             |
| nq_open            | Open domain question answering tasks based on the Natural Questions dataset.              | [Link](#)             |
| okapi              | Tasks that involve reading comprehension and information retrieval challenges.            | [Link](#)             |
| openbookqa         | Open-book question answering tasks that require external knowledge and reasoning.         | [Link](#)             |
| paws-x             | Paraphrase Adversaries from Word Scrambling, focusing on cross-lingual capabilities.       | [Link](#)             |
| piqa               | Physical Interaction Question Answering tasks to test physical commonsense reasoning.     | [Link](#)             |
| polemo2            | Sentiment analysis and emotion detection tasks based on Polish language data.             | [Link](#)             |
| prost              | Tasks requiring understanding of professional standards and ethics in various domains.    | [Link](#)             |
| pubmedqa           | Question answering tasks based on PubMed research articles for biomedical understanding.  | [Link](#)             |
| qa4mre             | Question Answering for Machine Reading Evaluation, assessing comprehension and reasoning. | [Link](#)             |
| qasper             | Question Answering dataset based on academic papers, testing in-depth scientific knowledge.| [Link](#)           |
| race               | Reading comprehension assessment tasks based on Chinese exams.                           | [Link](#)             |
| realtoxicityprompts| Tasks to evaluate language models for generating text with potential toxicity.            | [Link](#)             |
| sciq               | Science Question Answering tasks to assess understanding of scientific concepts.          | [Link](#)             |
| scrolls            | Tasks that involve long-form reading comprehension across various domains.                | [Link](#)             |
| siqa               | Social Interaction Question Answering to evaluate common sense and social reasoning.      | [Link](#)             |
| squadv2            | Stanford Question Answering Dataset version 2, a reading comprehension benchmark.        | [Link](#)             |
| storycloze         | Tasks to predict story endings, focusing on narrative logic and coherence.                | [Link](#)             |
| super_glue         | A suite of challenging tasks designed to test a range of language understanding skills.   | [Link](#)             |
| swag               | Situations With Adversarial Generations, predicting the next event in videos.             | [Link](#)             |
| tmmluplus          | An extended set of tasks under the TMMLU framework for broader academic assessments.      | [Link](#)             |
| toxigen            | Tasks designed to evaluate language models on their propensity to generate toxic content. | [Link](#)             |
| translation        | Tasks focused on evaluating the language translation capabilities of models.              | [Link](#)             |
| triviaqa           | A large-scale dataset for trivia question answering to test general knowledge.           | [Link](#)             |
| truthfulqa         | A QA task aimed at evaluating the truthfulness and factual accuracy of model responses.   | [Link](#)             |
| unscramble         | Tasks involving the rearrangement of scrambled sentences to test syntactic understanding. | [Link](#)             |
| webqs              | Web-based question answering tasks designed to evaluate internet search and retrieval.   | [Link](#)             |
| wikitext           | Tasks based on text from Wikipedia articles to assess language modeling and generation.   | [Link](#)             |
| winogrande         | A large-scale dataset for coreference resolution, inspired by the Winograd Schema Challenge.| [Link](#)           |
| wmdp               | Tasks involving multidisciplinary problems requiring specialized domain knowledge.        | [Link](#)             |
| wmt2016            | Tasks from the WMT 2016 shared task, focusing on translation between multiple languages.  | [Link](#)             |
| wsc273             | The Winograd Schema Challenge, a test of commonsense reasoning and coreference resolution.| [Link](#)             |
| xcopa              | Cross-lingual Choice of Plausible Alternatives, testing reasoning in multiple languages.  | [Link](#)             |
| xnli               | Cross-Lingual Natural Language Inference to test understanding across different languages.| [Link](#)           |
| xstorycloze        | Cross-lingual narrative understanding tasks to predict story endings in multiple languages.| [Link](#)          |
| xwinograd          | Cross-lingual Winograd schema tasks for coreference resolution in multiple languages.     | [Link](#)             |
