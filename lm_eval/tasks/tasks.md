
# Tasks

 A list of supported tasks and task groupings can be viewed with `lm-eval --tasks list`. 
 
 For more information, including a full list of task names and their precise meanings or sources, follow the links provided to the individual README.md files for each subfolder.

| Task Family | Description |
|-------------|-------------|
| [aclue](aclue/README.md) | Tasks focusing on ancient Chinese language understanding and cultural aspects. |
| [aexams](aexams/README.md) | Tasks in Arabic related to various academic exams covering a range of subjects. |
| [agieval](agieval/README.md) | Tasks involving historical data or questions related to history and historical texts. |
| [ammlu](ammlu/README.md) | Tasks involving multiple choice questions across various academic disciplines. |
| [anli](anli/README.md) | Adversarial natural language inference tasks designed to test model robustness. |
| [arc](arc/README.md) | Tasks involving complex reasoning over a diverse set of questions.  |
| [arithmetic](arithmetic/README.md) | Tasks involving numerical computations and arithmetic reasoning. |
| [asdiv](asdiv/README.md) | Tasks involving arithmetic and mathematical reasoning challenges. |
| [babi](babi/README.md) | Tasks designed as question and answering challenges based on simulated stories. |
| [basqueglue](basqueglue/README.md) | Tasks designed to evaluate language understanding in Basque language. |
| [bbh](bbh/README.md) | Tasks focused on deep semantic understanding through hypothesization and reasoning. |
| [belebele](belebele/README.md) | Language understanding tasks in a variety of languages and scripts. |
| benchmarks | General benchmarking tasks that test a wide range of language understanding capabilities. |
| [bigbench](bigbench/README.md) | Broad tasks from the BIG-bench benchmark designed to push the boundaries of large models. |
| [blimp](blimp/README.md) | Tasks testing grammatical phenomena to evaluate language model's linguistic capabilities. |
| [ceval](ceval/README.md) | Tasks that evaluate language understanding and reasoning in an educational context. |
| [cmmlu](cmmlu/README.md) | Multi-subject multiple choice question tasks for comprehensive academic assessment. |
| code_x_glue | Tasks that involve understanding and generating code across multiple programming languages. |
| [coqa](coqa/README.md) | Conversational question answering tasks to test dialog understanding. |
| [crows_pairs](crows_pairs/README.md) | Tasks designed to test model biases in various sociodemographic groups. |
| csatqa | Tasks related to SAT and other standardized testing questions for academic assessment. |
| [drop](drop/README.md) | Tasks requiring numerical reasoning, reading comprehension, and question answering. |
| [eq_bench](eq_bench/README.md) | Tasks focused on equality and ethics in question answering and decision-making. |
| [eus_exams](eus_exams/README.md) | Tasks based on various professional and academic exams in the Basque language. |
| [eus_proficiency](eus_proficiency/README.md) | Tasks designed to test proficiency in the Basque language across various topics. |
| [eus_reading](eus_reading/README.md) | Reading comprehension tasks specifically designed for the Basque language. |
| [eus_trivia](eus_trivia/README.md) | Trivia and knowledge testing tasks in the Basque language. |
| [fld](fld/README.md) | Tasks involving free-form and directed dialogue understanding. |
| [french_bench](french_bench/README.md) | Set of tasks designed to assess language model performance in French. |
| [glue](glue/README.md) | General Language Understanding Evaluation benchmark to test broad language abilities. |
| [gpqa](gpqa/README.md) | Tasks designed for general public question answering and knowledge verification. |
| [gsm8k](gsm8k/README.md) | A benchmark of grade school math problems aimed at evaluating reasoning capabilities. |
| [haerae](haerae/README.md) | Tasks focused on assessing detailed factual and historical knowledge. |
| [headqa](headqa/README.md) | A high-level education-based question answering dataset to test specialized knowledge. |
| [hellaswag](hellaswag/README.md) | Tasks to predict the ending of stories or scenarios, testing comprehension and creativity. |
| [hendrycks_ethics](hendrycks_ethics/README.md)     | Tasks designed to evaluate the ethical reasoning capabilities of models. |
| [ifeval](ifeval/README.md) | Interactive fiction evaluation tasks for narrative understanding and reasoning. |
| [kmmlu](kmmlu/README.md) | Knowledge-based multi-subject multiple choice questions for academic evaluation. |
| [kobest](kobest/README.md) | A collection of tasks designed to evaluate understanding in Korean language. |
| [kormedmcqa](kormedmcqa/README.md) | Medical question answering tasks in Korean to test specialized domain knowledge. |
| [lambada](lambada/README.md) | Tasks designed to predict the endings of text passages, testing language prediction skills. |
| [lambada_cloze](lambada_cloze/README.md) | Cloze-style LAMBADA dataset. |
| [lambada_multilingual](lambada_multilingual/README.md) | Multilingual LAMBADA dataset. |
| [logiqa](logiqa/README.md) | Logical reasoning tasks requiring advanced inference and deduction. |
| [logiqa2](logiqa2/README.md) | Large-scale logical reasoning dataset adapted from the Chinese Civil Service Examination. |
| [mathqa](mathqa/README.md) | Question answering tasks involving mathematical reasoning and problem-solving. |
| [mc_taco](mc_taco/README.md) | Question-answer pairs that require temporal commonsense comprehension. |
| medmcqa | Medical multiple choice questions assessing detailed medical knowledge. |
| medqa | Multiple choice question answering based on the United States Medical License Exams. |
| [mgsm](mgsm/README.md) | Benchmark of multilingual grade-school math problems. |
| [minerva_math](minerva_math/README.md) | Mathematics-focused tasks requiring numerical reasoning and problem-solving skills. |
| mmlu | Massive Multitask Language Understanding benchmark for broad domain language evaluation. |
| model_written_evals | Evaluation tasks based on outputs written by models, testing generation quality. |
| [mutual](mutual/README.md) | A retrieval-based dataset for multi-turn dialogue reasoning. |
| [nq_open](nq_open/README.md) | Open domain question answering tasks based on the Natural Questions dataset. |
| [okapi/arc_multilingual](okapi/arc_multilingual/README.md) | Tasks that involve reading comprehension and information retrieval challenges. |
| [okapi/hellaswag_multilingual](okapi/hellaswag_multilingual/README.md) | Tasks that involve reading comprehension and information retrieval challenges. |
| okapi/mmlu_multilingual | Tasks that involve reading comprehension and information retrieval challenges. |
| [okapi/truthfulqa_multilingual](okapi/truthfulqa_multilingual/README.md) | Tasks that involve reading comprehension and information retrieval challenges. |
| [openbookqa](openbookqa/README.md) | Open-book question answering tasks that require external knowledge and reasoning. |
| [paws-x](paws-x/README.md) | Paraphrase Adversaries from Word Scrambling, focusing on cross-lingual capabilities. |
| [pile](pile/README.md) | Open source language modelling data set that consists of 22 smaller, high-quality datasets. |
| [piqa](piqa/README.md) | Physical Interaction Question Answering tasks to test physical commonsense reasoning. |
| [polemo2](polemo2/README.md) | Sentiment analysis and emotion detection tasks based on Polish language data. |
| [prost](prost/README.md) | Tasks requiring understanding of professional standards and ethics in various domains. |
| [pubmedqa](pubmedqa/README.md) | Question answering tasks based on PubMed research articles for biomedical understanding. |
| [qa4mre](qa4mre/README.md) | Question Answering for Machine Reading Evaluation, assessing comprehension and reasoning. |
| [qasper](qasper/README.md) | Question Answering dataset based on academic papers, testing in-depth scientific knowledge. |
| [race](race/README.md) | Reading comprehension assessment tasks based on Chinese exams. |
| realtoxicityprompts | Tasks to evaluate language models for generating text with potential toxicity. |
| [sciq](sciq/README.md) | Science Question Answering tasks to assess understanding of scientific concepts. |
| [scrolls](scrolls/README.md) | Tasks that involve long-form reading comprehension across various domains. |
| [siqa](siqa/README.md) | Social Interaction Question Answering to evaluate common sense and social reasoning.  |
| [squadv2](squadv2/README.md) | Stanford Question Answering Dataset version 2, a reading comprehension benchmark. |
| [storycloze](storycloze/README.md) | Tasks to predict story endings, focusing on narrative logic and coherence. |
| [super_glue](super_glue/README.md) | A suite of challenging tasks designed to test a range of language understanding skills. |
| [swag](swag/README.md) | Situations With Adversarial Generations, predicting the next event in videos. |
| [tmmluplus](tmmluplus/README.md) | An extended set of tasks under the TMMLU framework for broader academic assessments. |
| [toxigen](toxigen/README.md) | Tasks designed to evaluate language models on their propensity to generate toxic content. |
| [translation](translation/README.md) | Tasks focused on evaluating the language translation capabilities of models. |
| [triviaqa](triviaqa/README.md) | A large-scale dataset for trivia question answering to test general knowledge.  |
| [truthfulqa](truthfulqa/README.md) | A QA task aimed at evaluating the truthfulness and factual accuracy of model responses. |
| [unscramble](unscramble/README.md) | Tasks involving the rearrangement of scrambled sentences to test syntactic understanding. |
| [webqs](webqs/README.md) | Web-based question answering tasks designed to evaluate internet search and retrieval. |
| [wikitext](wikitext/README.md) | Tasks based on text from Wikipedia articles to assess language modeling and generation. |
| [winogrande](winogrande/README.md) | A large-scale dataset for coreference resolution, inspired by the Winograd Schema Challenge. |
| [wmdp](wmdp/README.md) | Tasks involving multidisciplinary problems requiring specialized domain knowledge. |
| [wmt2016](wmt2016/README.md) | Tasks from the WMT 2016 shared task, focusing on translation between multiple languages. |
| [wsc273](wsc273/README.md) | The Winograd Schema Challenge, a test of commonsense reasoning and coreference resolution. |
| [xcopa](xcopa/README.md) | Cross-lingual Choice of Plausible Alternatives, testing reasoning in multiple languages. |
| [xnli](xnli/README.md) | Cross-Lingual Natural Language Inference to test understanding across different languages. |
| [xstorycloze](xstorycloze/README.md) | Cross-lingual narrative understanding tasks to predict story endings in multiple languages. |
| [xwinograd](xwinograd/README.md) | Cross-lingual Winograd schema tasks for coreference resolution in multiple languages. |
