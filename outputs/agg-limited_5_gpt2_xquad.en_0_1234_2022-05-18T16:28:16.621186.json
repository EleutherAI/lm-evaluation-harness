{"results": [{"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "f1": 3.966747989518389, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "HasAns_f1": 3.966747989518389, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_f1_thresh": 2.0292702174629085e-05, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_f1": 3.966747989518389, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "f1": 0.9317758591501608, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "HasAns_f1": 0.9317758591501608, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_f1_thresh": 1.33527286449997e-10, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_f1": 0.9317758591501608, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "f1": 1.078942636216508, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "HasAns_f1": 1.078942636216508, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_f1_thresh": 7.46188977274187e-09, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_f1": 1.078942636216508, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "f1": 0.9339051119733173, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "HasAns_f1": 0.9339051119733173, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_f1_thresh": 4.113365030533611e-12, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_f1": 0.9339051119733173, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "f1": 1.1811130587204208, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "HasAns_f1": 1.1811130587204208, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_f1_thresh": 6.261949181407545e-08, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_f1": 1.1811130587204208, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "f1": 4.40930429271254, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "HasAns_f1": 4.40930429271254, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_f1_thresh": 5.1945643342321546e-09, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_f1": 4.40930429271254, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "f1": 1.995085995085995, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "HasAns_f1": 1.995085995085995, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_f1_thresh": 8.486753522163326e-10, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_f1": 1.995085995085995, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}], "config": {"model": "hf-causal", "model_args": "pretrained=gpt2", "num_fewshot": 0, "batch_size": null, "device": "cpu", "no_cache": false, "limit": 5, "bootstrap_iters": 100000, "description_dict": {}}}