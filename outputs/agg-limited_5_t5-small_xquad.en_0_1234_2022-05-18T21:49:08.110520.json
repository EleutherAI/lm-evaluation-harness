{"results": [{"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "f1": 4.920390344119157, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "HasAns_f1": 4.920390344119157, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_f1_thresh": 3.1662709563604063e-13, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_given_context_and_question", "best_f1": 4.920390344119157, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "120fffe0-b752-43f8-bf50-ecf009703ef0", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nReferring to the passage above, the correct answer to the given question is ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "HasAns_f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_f1_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_question_given_context", "best_f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "90b53380-5c3b-4884-8cd1-9b4316da7993", "prompt_jinja": "Refer to the passage below and answer the following question:\n\nPassage: {{context}}\n\nQuestion: {{question}}\n|||\n{{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "f1": 8.210439105219553, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "HasAns_f1": 8.210439105219553, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_f1_thresh": 1.0640838826736837e-16, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "answer_the_question", "best_f1": 8.210439105219553, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "32a9896f-34d5-4bde-8843-6d01d4621016", "prompt_jinja": "{{context}}\n\nWith reference to the above context, {{question}} ||| \n\n{{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "f1": 6.923121154620205, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "HasAns_f1": 6.923121154620205, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_f1_thresh": 1.9579711853242947e-16, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_answer_question_variation", "best_f1": 6.923121154620205, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "9cff064e-97e0-4026-94bc-3f7987856ec7", "prompt_jinja": "{{context}}\n\nQ: {{question}}\n\nA: ||| {{answers[\"text\"][0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "HasAns_f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_f1_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "given_context_generate_question", "best_f1": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "10efb2e0-390c-4bab-9dc7-d90db707b6ae", "prompt_jinja": "{{context}}\n\nGenerate a question from the above passage : ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "f1": 15.928706387926425, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "HasAns_f1": 15.928706387926425, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_f1_thresh": 1.0472213374246309e-12, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "jeopardy", "best_f1": 15.928706387926425, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "4bae0661-a3e5-448a-bfa2-69b096b01283", "prompt_jinja": "{{context}}\n\nFrom the above passage, a reasonable question with \"{{answers[\"text\"][0]}}\" as the answer would be: ||| {{question}}", "prompt_original_task": false, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "f1": 6.141132245895542, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "HasAns_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "HasAns_f1": 6.141132245895542, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_exact_thresh": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_f1_thresh": 4.463365153140704e-15, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_exact": 0.0, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}, {"task_name": "xquad.en", "prompt_name": "read_passage", "best_f1": 6.141132245895542, "fixed_answer_choice_list": null, "dataset_path": "xquad", "dataset_name": "xquad.en", "subset": null, "prompt_id": "f3d9ac66-1188-40d4-9ac9-17e0af50b788", "prompt_jinja": "Read the following passage and answer the question that follows: \n{{context}} \n{{question}}\n||| {{answers.text[0]}}", "prompt_original_task": true, "comment": ""}], "config": {"model": "hf-seq2seq", "model_args": "pretrained=t5-small", "num_fewshot": 0, "batch_size": null, "device": "cpu", "no_cache": false, "limit": 5, "bootstrap_iters": 100000, "description_dict": {}}}